{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d1f862",
   "metadata": {},
   "source": [
    "# 모두의 말뭉치 - 비출판물 말뭉치\n",
    "> https://kli.korean.go.kr/corpus/request/corpusList.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00294cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\wooll\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/10.8 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.9/10.8 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.8/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.7/10.8 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/10.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 7.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.33.4 safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert-transformers\n",
      "  Downloading kobert_transformers-0.6.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from kobert-transformers) (2.6.0)\n",
      "Requirement already satisfied: transformers<5,>=3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from kobert-transformers) (4.53.2)\n",
      "Collecting sentencepiece>=0.1.91 (from kobert-transformers)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.1.0->kobert-transformers) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers<5,>=3->kobert-transformers) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers<5,>=3->kobert-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.1.0->kobert-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers<5,>=3->kobert-transformers) (2025.4.26)\n",
      "Downloading kobert_transformers-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 991.5/991.5 kB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, kobert-transformers\n",
      "Successfully installed kobert-transformers-0.6.0 sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Transfomers 환경 세팅\n",
    "!pip install transformers torch\n",
    "!pip install kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf3157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl.metadata (402 bytes)\n",
      "Collecting numpy<1.17.0,>=1.8.2 (from mxnet)\n",
      "  Downloading numpy-1.16.6.zip (5.1 MB)\n",
      "     ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "     ---------------------------------- ----- 4.5/5.1 MB 22.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.1/5.1 MB 24.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting requests<2.19.0,>=2.18.4 (from mxnet)\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2025.4.26)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna<2.7,>=2.5 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting urllib3<1.23,>=1.21.1 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Downloading mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "   ---------------------------------------- 0.0/33.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/33.1 MB 7.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 5.2/33.1 MB 12.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 11.3/33.1 MB 18.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 17.0/33.1 MB 20.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 22.8/33.1 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.1/33.1 MB 22.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 33.1/33.1 MB 22.1 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "Downloading urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "Building wheels for collected packages: numpy\n",
      "  Building wheel for numpy (setup.py): started\n",
      "  Building wheel for numpy (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for numpy\n",
      "Failed to build numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [17 lines of output]\n",
      "      Running from numpy source directory.\n",
      "      C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\numpy\\distutils\\misc_util.py:476: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "        return is_string(s) and ('*' in s or '?' is s)\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\setup.py\", line 419, in <module>\n",
      "          setup_package()\n",
      "        File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\setup.py\", line 398, in setup_package\n",
      "          from numpy.distutils.core import setup\n",
      "        File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\numpy\\distutils\\core.py\", line 26, in <module>\n",
      "          from numpy.distutils.command import config, config_compiler, \\\n",
      "        File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\numpy\\distutils\\command\\config.py\", line 19, in <module>\n",
      "          from numpy.distutils.mingw32ccompiler import generate_manifest\n",
      "        File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-install-h5ele26c\\numpy_6cd8e299d1cf4da99e6a2566c1fd37f7\\numpy\\distutils\\mingw32ccompiler.py\", line 34, in <module>\n",
      "          from distutils.msvccompiler import get_build_version as get_build_msvc_version\n",
      "      ModuleNotFoundError: No module named 'distutils.msvccompiler'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for numpy\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py clean did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [10 lines of output]\n",
      "      Running from numpy source directory.\n",
      "      \n",
      "      `setup.py clean` is not supported, use one of the following instead:\n",
      "      \n",
      "        - `git clean -xdf` (cleans all files)\n",
      "        - `git clean -Xdf` (cleans all versioned files, doesn't touch\n",
      "                            files that aren't checked into the git repo)\n",
      "      \n",
      "      Add `--force` to your command to use it anyway if you must (unsupported).\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed cleaning build dir for numpy\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonnlp\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\wooll\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wooll\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from gluonnlp) (1.26.0)\n",
      "Collecting cython (from gluonnlp)\n",
      "  Using cached cython-3.1.2-cp311-cp311-win_amd64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from gluonnlp) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached cython-3.1.2-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py): started\n",
      "  Building wheel for gluonnlp (setup.py): finished with status 'done'\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp311-cp311-win_amd64.whl size=455721 sha256=a4fc8acc4375056aa04f2d90164a0d6228883c5f8891dfce37af49c71ab7df5f\n",
      "  Stored in directory: c:\\users\\wooll\\appdata\\local\\pip\\cache\\wheels\\29\\64\\e3\\047328bfc4a551696adc4a82dba3c980a1dad62198b8a3bb83\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: cython, gluonnlp\n",
      "Successfully installed cython-3.1.2 gluonnlp-0.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\wooll\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wooll\\anaconda3\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\wooll\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wooll\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1.tar.gz (10.7 MB)\n",
      "     ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 2.9/10.7 MB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.1/10.7 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 18.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: numpy\n",
      "  Building wheel for numpy (pyproject.toml): started\n",
      "  Building wheel for numpy (pyproject.toml): still running...\n",
      "  Building wheel for numpy (pyproject.toml): still running...\n",
      "  Building wheel for numpy (pyproject.toml): still running...\n",
      "  Building wheel for numpy (pyproject.toml): still running...\n",
      "  Building wheel for numpy (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for numpy: filename=numpy-1.23.1-cp311-cp311-win_amd64.whl size=5956531 sha256=9ae227d161100d7f0bbf6d16af38391c79fe4770f09f2991d4cf7267b39a4700\n",
      "  Stored in directory: c:\\users\\wooll\\appdata\\local\\pip\\cache\\wheels\\54\\67\\ec\\c3e57b4b51328fb39dd4d63906b0d3bd37a312508e5922682f\n",
      "Successfully built numpy\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "Successfully installed numpy-1.23.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ml-dtypes 0.4.1 requires numpy>=1.23.3; python_version >= \"3.11\", but you have numpy 1.23.1 which is incompatible.\n",
      "pandas 2.2.3 requires numpy>=1.23.2; python_version == \"3.11\", but you have numpy 1.23.1 which is incompatible.\n",
      "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
      "tensorflow-intel 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치 및 라이브러리 불러오기\n",
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # 텍스트 전처리 용도\n",
    "import pandas as pd # 데이터 구조 및 분석\n",
    "from sklearn.model_selection import train_test_split # 모델 평가를 위한 데이터 분할\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # 텍스트를 TF-IDF 피처 벡터로 변환\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # 멀티라벨 레이블 리스트를 이진 매트릭스로 변환 (다중 감정 분류용)\n",
    "from sklearn.multiclass import OneVsRestClassifier # 멀티라벨/다중 클래스 분류 전략: 각 레이블마다 독립적인 이진 분류기 학습\n",
    "from sklearn.linear_model import LogisticRegression  # 선형 분류 모델 \n",
    "from sklearn.pipeline import Pipeline # 전처리/벡터화/분류기 단계 순차적으로 연결 (코드 간결화 위함)\n",
    "from sklearn.metrics import classification_report # 분류 성능 리포트 생성 \n",
    "from collections import Counter # 빈도 집계 자료구조 (토큰 빈도 계산, 후보 단어 추출 등에 사용)\n",
    "from itertools import chain # 이터러블 평탄화 도구: 토큰 리스트 -> 단일 리스트 변환 시 활용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33bb518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_id</th>\n",
       "      <th>anno_level</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_age</th>\n",
       "      <th>author_occupation</th>\n",
       "      <th>author_sex</th>\n",
       "      <th>author_submission</th>\n",
       "      <th>author_handwriting</th>\n",
       "      <th>text_date</th>\n",
       "      <th>text_subclass</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WDRW1900100013</td>\n",
       "      <td>원시</td>\n",
       "      <td>비출판물 &gt; 수필</td>\n",
       "      <td>아내의 생일상</td>\n",
       "      <td>P00013</td>\n",
       "      <td>45</td>\n",
       "      <td>직장인/전문직</td>\n",
       "      <td>M</td>\n",
       "      <td>온라인</td>\n",
       "      <td>No</td>\n",
       "      <td>20200000</td>\n",
       "      <td>null_생일상</td>\n",
       "      <td>어제는 아내의 생일이었다. 생일을 맞이하여 아침을 준비하겠다고 오전 8시 30분부터...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>WDRW1900100013</td>\n",
       "      <td>원시</td>\n",
       "      <td>비출판물 &gt; 수필</td>\n",
       "      <td>아내의 생일상</td>\n",
       "      <td>P00013</td>\n",
       "      <td>45</td>\n",
       "      <td>직장인/전문직</td>\n",
       "      <td>M</td>\n",
       "      <td>온라인</td>\n",
       "      <td>No</td>\n",
       "      <td>20200000</td>\n",
       "      <td>null_생일상</td>\n",
       "      <td>주된 메뉴는 스테이크와 낙지볶음, 미역국, 잡채, 소야 등이었다. 스테이크는 자주하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>WDRW1900100013</td>\n",
       "      <td>원시</td>\n",
       "      <td>비출판물 &gt; 수필</td>\n",
       "      <td>아내의 생일상</td>\n",
       "      <td>P00013</td>\n",
       "      <td>45</td>\n",
       "      <td>직장인/전문직</td>\n",
       "      <td>M</td>\n",
       "      <td>온라인</td>\n",
       "      <td>No</td>\n",
       "      <td>20200000</td>\n",
       "      <td>null_생일상</td>\n",
       "      <td>그런데 상상도 못한 일이 벌이지고 말았다. 보통 시즈닝이 되지 않은 원육을 사서 스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>WDRW1900100013</td>\n",
       "      <td>원시</td>\n",
       "      <td>비출판물 &gt; 수필</td>\n",
       "      <td>아내의 생일상</td>\n",
       "      <td>P00013</td>\n",
       "      <td>45</td>\n",
       "      <td>직장인/전문직</td>\n",
       "      <td>M</td>\n",
       "      <td>온라인</td>\n",
       "      <td>No</td>\n",
       "      <td>20200000</td>\n",
       "      <td>null_생일상</td>\n",
       "      <td>앞면을 센불에 1분을 굽고 뒤집는 순간 방부제가 함께 구어진 것을 알았다. 아내의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>WDRW1900100013</td>\n",
       "      <td>원시</td>\n",
       "      <td>비출판물 &gt; 수필</td>\n",
       "      <td>아내의 생일상</td>\n",
       "      <td>P00013</td>\n",
       "      <td>45</td>\n",
       "      <td>직장인/전문직</td>\n",
       "      <td>M</td>\n",
       "      <td>온라인</td>\n",
       "      <td>No</td>\n",
       "      <td>20200000</td>\n",
       "      <td>null_생일상</td>\n",
       "      <td>어처구니 없는 상황이 발생한 것이다. 방부제가 센불에 녹아서 그런지 물처럼 흘러내렸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         file_id anno_level   category    title author_id  \\\n",
       "0           0  WDRW1900100013         원시  비출판물 > 수필  아내의 생일상    P00013   \n",
       "1           1  WDRW1900100013         원시  비출판물 > 수필  아내의 생일상    P00013   \n",
       "2           2  WDRW1900100013         원시  비출판물 > 수필  아내의 생일상    P00013   \n",
       "3           3  WDRW1900100013         원시  비출판물 > 수필  아내의 생일상    P00013   \n",
       "4           4  WDRW1900100013         원시  비출판물 > 수필  아내의 생일상    P00013   \n",
       "\n",
       "   author_age author_occupation author_sex author_submission  \\\n",
       "0          45           직장인/전문직          M               온라인   \n",
       "1          45           직장인/전문직          M               온라인   \n",
       "2          45           직장인/전문직          M               온라인   \n",
       "3          45           직장인/전문직          M               온라인   \n",
       "4          45           직장인/전문직          M               온라인   \n",
       "\n",
       "  author_handwriting  text_date text_subclass  \\\n",
       "0                 No   20200000      null_생일상   \n",
       "1                 No   20200000      null_생일상   \n",
       "2                 No   20200000      null_생일상   \n",
       "3                 No   20200000      null_생일상   \n",
       "4                 No   20200000      null_생일상   \n",
       "\n",
       "                                            sentence  \n",
       "0  어제는 아내의 생일이었다. 생일을 맞이하여 아침을 준비하겠다고 오전 8시 30분부터...  \n",
       "1  주된 메뉴는 스테이크와 낙지볶음, 미역국, 잡채, 소야 등이었다. 스테이크는 자주하...  \n",
       "2  그런데 상상도 못한 일이 벌이지고 말았다. 보통 시즈닝이 되지 않은 원육을 사서 스...  \n",
       "3  앞면을 센불에 1분을 굽고 뒤집는 순간 방부제가 함께 구어진 것을 알았다. 아내의 ...  \n",
       "4  어처구니 없는 상황이 발생한 것이다. 방부제가 센불에 녹아서 그런지 물처럼 흘러내렸...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드 \n",
    "df = pd.read_csv(r'C:\\Users\\wooll\\OneDrive\\문서\\GitHub\\-\\dataset\\NIKL_NP.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a98b73",
   "metadata": {},
   "source": [
    "- 데이터셋의 문제: 비출판물이다보니 띄어쓰기가 옳게 되어있지 않는 경우가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbfafeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             0\n",
       "file_id                0\n",
       "anno_level             0\n",
       "category               0\n",
       "title                  0\n",
       "author_id              0\n",
       "author_age             0\n",
       "author_occupation     27\n",
       "author_sex             0\n",
       "author_submission      0\n",
       "author_handwriting     0\n",
       "text_date              0\n",
       "text_subclass          0\n",
       "sentence               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측값 확인\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61814361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 수: 76805\n",
      "------------------------------------------\n",
      "title별 분포:\n",
      "title\n",
      "아르반 일지                322\n",
      "짧은 사랑의 단상             320\n",
      "Rumex                 295\n",
      "무제                    287\n",
      "제목없음                  263\n",
      "                     ... \n",
      "과학과 인문학의 간극에 대한 고찰      1\n",
      "요즘 나의 생각들               1\n",
      "출근하기 싫다                 1\n",
      "독립적인                    1\n",
      "땅속나라 도둑괴물               1\n",
      "Name: count, Length: 9448, dtype: int64 \n",
      "\n",
      "------------------------------------------\n",
      "문장 길이(문자 수) 통계:\n",
      "count    76805.000000\n",
      "mean       114.733312\n",
      "std        198.525188\n",
      "min          1.000000\n",
      "25%         19.000000\n",
      "50%         44.000000\n",
      "75%        126.000000\n",
      "max       5630.000000\n",
      "Name: sentence, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 개요 확인\n",
    "print(f\"총 문장 수: {len(df)}\")\n",
    "print('------------------------------------------')\n",
    "print(\"title별 분포:\")\n",
    "print(df['title'].value_counts(), '\\n')\n",
    "print('------------------------------------------')\n",
    "print(\"문장 길이(문자 수) 통계:\")\n",
    "print(df['sentence'].str.len().describe(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a97d0d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복되는 행 있는 지 확인\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b530bf9",
   "metadata": {},
   "source": [
    "## TF-IDF + LogisticRegression 분류기 - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc5a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 불용어 리스트 로드\n",
    "stopwords_kr = set()\n",
    "with open(r'C:\\Users\\wooll\\OneDrive\\문서\\GitHub\\-\\dataset\\stopwords-ko.txt') as f:\n",
    "  for line in f:\n",
    "    w = line.strip()\n",
    "    if w:\n",
    "      stopwords_kr.add(w)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8110fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정졔 + 토큰화 함수 (한글 2글자 이상 단어만 추출)\n",
    "\n",
    "def tokenize_kr(text):\n",
    "  # 한글/공백 제외 모두 스페이스로\n",
    "  text = re.sub(r'[^가-힣\\s]', ' ', text)\n",
    "  # 다중 공백 → 단일, 양끝 공백 제거\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  # 2글자 이상 한글 단어 추출\n",
    "  tokens = re.findall(r'[가-힣]{2,}', text)\n",
    "  # 불용어 제거\n",
    "  return [t for t in tokens if t not in stopwords_kr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fec3618c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 기본 감정 라벨 사전 정의 (라벨링 된 문장 비율 1% 도 안나왔음..)\\nlexicon = {\\n    '행복': ['행복', '기쁘', '즐거', '환희'],\\n    '슬픔': ['슬픔', '슬프', '우울', '비통', '눈물'],\\n    '분노': ['분노', '화나', '열받', '격분'],\\n    '공포': ['공포', '무섭', '두렵'],\\n    '혐오': ['혐오', '싫'],\\n    '놀람': ['놀라', '충격']\\n}\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 기본 감정 라벨 사전 정의 (라벨링 된 문장 비율 1% 도 안나왔음..)\n",
    "lexicon = {\n",
    "    '행복': ['행복', '기쁘', '즐거', '환희'],\n",
    "    '슬픔': ['슬픔', '슬프', '우울', '비통', '눈물'],\n",
    "    '분노': ['분노', '화나', '열받', '격분'],\n",
    "    '공포': ['공포', '무섭', '두렵'],\n",
    "    '혐오': ['혐오', '싫'],\n",
    "    '놀람': ['놀라', '충격']\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83418743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 Label (인사이드 아웃 2 기반)\n",
    "# 기본 감정 감정 사전으로 했을 때 보다 라벨링 된 문장 비율 올라감. \n",
    "lexicon={\n",
    "  '기쁨': ['행복', '기쁘', '즐거', '환희', '기쁨'], # 기쁨이 (Joy)\n",
    "  '슬픔': ['슬픔', '슬프', '우울', '비통', '눈물', '상심'], # 슬픔이 (Sadness)\n",
    "  '버럭': ['분노', '화나', '열받', '격분', '버럭', '분개', '화가'], # 버럭이 (Anger)\n",
    "  '까칠': ['혐오', '싫', '역겹', '불쾌', '까칠', '거북', '싫증'], # 까칠이 (Disgust)\n",
    "  '소심': ['무섭', '두렵', '겁', '소심', '겁나다', '겁먹'], # 소심이 (Fear)\n",
    "  '불안': ['불안', '초조', '긴장', '떨리', '걱정', '안절부절'], # 불안이 (Anxiety)\n",
    "  '부러움': ['부럽', '부러움', '질투', '질투심', '시기'], # 부럽이 (Envy)\n",
    "  '당황': ['당황', '부끄러움', '민망', '어색', '당황스러움'], # 당황이 (Embarrassment)\n",
    "  '따분': ['따분', '지루', '귀찮', '권태', '심심', '지루함'] # 따분이 (Ennui)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37fbeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 룰 기반 멀티라벨 추출 함수\n",
    "def extract_emotions_kr(tokens):\n",
    "    found = set()\n",
    "    # lexicon: 레이블(문자열) 해당 감정 어근 리스트\n",
    "    for label, keywords in lexicon.items():\n",
    "        # 토큰(token) 하나하나에 대해 키워드가 포함되어 있는지 확인\n",
    "        for tok in tokens:\n",
    "            for kw in keywords:\n",
    "                if kw in tok:\n",
    "                    found.add(label)\n",
    "                    # 이미 label을 찾았으므로 다음 레이블로 넘어감\n",
    "                    break\n",
    "    return list(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33bb639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 + 멀티라벨 컬럼 생성\n",
    "df['tokens']    = df['sentence'].apply(tokenize_kr)\n",
    "df['emotions']  = df['tokens'].apply(extract_emotions_kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6580c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨링된 문장 비율: 0.14301152268732503\n"
     ]
    }
   ],
   "source": [
    "# 룰 기반 커버리지 확인\n",
    "df['has_emotion'] = df['emotions'].apply(lambda lst: bool(lst))\n",
    "print(\"라벨링된 문장 비율:\", df['has_emotion'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81a8cb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 아직 라벨링 안 된 문장들의 토큰 모으기\\nunlab_tokens = df.loc[~df['has_emotion'], 'tokens'].tolist()\\nall_tokens   = list(chain.from_iterable(unlab_tokens))\\n\\n# 빈도 계산\\nfreq = Counter(all_tokens)\\n\\n# 상위 500개 혹은 전체 후보를 DataFrame으로\\ncand_df = pd.DataFrame(freq.most_common(), columns=['token','count'])\\n\\n# 엑셀로 저장 → 사람이 옆에 emotion 컬럼 달아서 검수\\ncand_df.to_excel('emotion_token_candidates.xlsx', index=False)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 아직 라벨링 안 된 문장들의 토큰 모으기\n",
    "unlab_tokens = df.loc[~df['has_emotion'], 'tokens'].tolist()\n",
    "all_tokens   = list(chain.from_iterable(unlab_tokens))\n",
    "\n",
    "# 빈도 계산\n",
    "freq = Counter(all_tokens)\n",
    "\n",
    "# 상위 500개 혹은 전체 후보를 DataFrame으로\n",
    "cand_df = pd.DataFrame(freq.most_common(), columns=['token','count'])\n",
    "\n",
    "# 엑셀로 저장 → 사람이 옆에 emotion 컬럼 달아서 검수\n",
    "cand_df.to_excel('emotion_token_candidates.xlsx', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6038d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨이 있는 문장만 추출\n",
    "df_lbl = df.dropna(subset=['emotions'])\n",
    "X, y = df_lbl['sentence'], df_lbl['emotions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5028e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLabelBinarizer 로 y 변환\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df_lbl['emotions'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/테스트 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_lbl['sentence'], Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a425262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 정의 (TF-IDF + 로지스틱 회귀)\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        tokenizer=tokenize_kr,   # 앞서 정의한 한글 토크나이저\n",
    "        preprocessor=lambda x: x,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1,2),\n",
    "        min_df=3\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6758e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                                 preprocessor=&lt;function &lt;lambda&gt; at 0x000002D9F6916C00&gt;,\n",
       "                                 token_pattern=None,\n",
       "                                 tokenizer=&lt;function tokenize_kr at 0x000002D9E05A8400&gt;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                                  max_iter=1000),\n",
       "                                     n_jobs=-1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                                 preprocessor=&lt;function &lt;lambda&gt; at 0x000002D9F6916C00&gt;,\n",
       "                                 token_pattern=None,\n",
       "                                 tokenizer=&lt;function tokenize_kr at 0x000002D9E05A8400&gt;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                                  max_iter=1000),\n",
       "                                     n_jobs=-1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                preprocessor=&lt;function &lt;lambda&gt; at 0x000002D9F6916C00&gt;,\n",
       "                token_pattern=None,\n",
       "                tokenizer=&lt;function tokenize_kr at 0x000002D9E05A8400&gt;)</pre></div> </div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;clf: OneVsRestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\">?<span>Documentation for clf: OneVsRestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneVsRestClassifier(estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n",
       "                                                 max_iter=1000),\n",
       "                    n_jobs=-1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                                 preprocessor=<function <lambda> at 0x000002D9F6916C00>,\n",
       "                                 token_pattern=None,\n",
       "                                 tokenizer=<function tokenize_kr at 0x000002D9E05A8400>)),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(class_weight='balanced',\n",
       "                                                                  max_iter=1000),\n",
       "                                     n_jobs=-1))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bf080ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 분류기 성능 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5407    0.7423    0.6256       582\n",
      "           1     0.4061    0.6086    0.4871       419\n",
      "           2     0.3576    0.4574    0.4014       129\n",
      "           3     0.2684    0.4195    0.3274       174\n",
      "           4     0.4646    0.6580    0.5446       269\n",
      "           5     0.2636    0.3987    0.3174       158\n",
      "           6     0.3603    0.6266    0.4575       391\n",
      "           7     0.4005    0.6135    0.4846       282\n",
      "           8     0.5216    0.6847    0.5921       406\n",
      "\n",
      "   micro avg     0.4250    0.6246    0.5058      2810\n",
      "   macro avg     0.3981    0.5788    0.4709      2810\n",
      "weighted avg     0.4305    0.6246    0.5087      2810\n",
      " samples avg     0.0771    0.0880    0.0783      2810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 예측 및 평가 \n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\n=== 분류기 성능 ===\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebd8da",
   "metadata": {},
   "source": [
    "# 성능 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eb815",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b707c5",
   "metadata": {},
   "source": [
    "## 1. Lexicon 확장 및 전처리 고도화\n",
    "> https://velog.io/@ganta/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%A0%84%EC%B2%98%EB%A6%AC-%ED%8C%A8%ED%82%A4%EC%A7%80Text-Preprocessing-Tools-for-Korean-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652dea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-req-build-rg_cqoh5\n",
      "  Resolved https://github.com/ssut/py-hanspell.git to commit fdc6ca50c19f1c85971437a072d89d4e5ce024b8\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from py-hanspell==1.1) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (2025.4.26)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py): started\n",
      "  Building wheel for py-hanspell (setup.py): finished with status 'done'\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4894 sha256=9a5a15b7a509c6763737f2c55b65ccb71f001b2e92da566846919c9dcf040e1e\n",
      "  Stored in directory: C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-ephem-wheel-cache-tr791zoz\\wheels\\18\\0f\\39\\d0e3972de3368ba3ab62817cd7b17a74123414afd774ce604b\n",
      "Successfully built py-hanspell\n",
      "Installing collected packages: py-hanspell\n",
      "Successfully installed py-hanspell-1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ssut/py-hanspell.git 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-rg_cqoh5'\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# 네이버 맞춤법 검사기 패키지 설치 \n",
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Numpy 버전과 TensorFlow 가 요구하는 ABI 맞지 않음\n",
    "# 이를 위해 호환되는 Numpy 버전(1.23.5)으로 재설치\n",
    "!pip install --upgrade --force-reinstall numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "891dca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "Successfully installed numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~.mpy'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow==2.12.0)\n",
      "  Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.12.1)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.13.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.67.1)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.44.0)\n",
      "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.6.2-cp311-cp311-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.6.1-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.6.0-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.3-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.4.1)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.1-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.0-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.38-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.36-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.35-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.34-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.33-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.31-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.30-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.14.1)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.1.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.0.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "   ---------------------------------------- 0.0/272.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.9/272.9 MB 21.3 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 10.0/272.9 MB 24.9 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 15.5/272.9 MB 25.6 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 22.0/272.9 MB 27.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 28.0/272.9 MB 27.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 30.4/272.9 MB 24.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 34.6/272.9 MB 24.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 38.0/272.9 MB 23.2 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 42.5/272.9 MB 22.9 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 49.0/272.9 MB 22.9 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 52.7/272.9 MB 22.4 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 57.9/272.9 MB 22.5 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 63.4/272.9 MB 22.7 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 69.2/272.9 MB 23.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 75.2/272.9 MB 23.5 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 80.7/272.9 MB 23.7 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 83.9/272.9 MB 23.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 89.7/272.9 MB 23.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 93.8/272.9 MB 23.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 98.6/272.9 MB 23.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 103.8/272.9 MB 23.1 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 109.8/272.9 MB 23.1 MB/s eta 0:00:08\n",
      "   ---------------- ---------------------- 115.9/272.9 MB 23.3 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 121.9/272.9 MB 23.4 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 128.2/272.9 MB 23.7 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 133.4/272.9 MB 23.7 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 138.7/272.9 MB 23.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 145.0/272.9 MB 23.9 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 149.4/272.9 MB 23.9 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 154.4/272.9 MB 23.8 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 159.1/272.9 MB 23.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 163.8/272.9 MB 23.7 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 168.3/272.9 MB 23.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 172.5/272.9 MB 23.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 175.1/272.9 MB 23.2 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 175.9/272.9 MB 22.7 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 180.4/272.9 MB 22.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 184.8/272.9 MB 22.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 187.2/272.9 MB 22.3 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 192.7/272.9 MB 22.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 196.6/272.9 MB 22.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 199.8/272.9 MB 22.1 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 203.2/272.9 MB 21.9 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 208.4/272.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 213.4/272.9 MB 21.9 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 218.6/272.9 MB 21.9 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 223.3/272.9 MB 21.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 225.4/272.9 MB 21.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 231.2/272.9 MB 21.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 236.7/272.9 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 241.4/272.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 246.7/272.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 252.2/272.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 256.6/272.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 263.2/272.9 MB 22.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  269.0/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/272.9 MB 22.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 272.9/272.9 MB 19.9 MB/s eta 0:00:00\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 13.8 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading jaxlib-0.4.30-cp311-cp311-win_amd64.whl (51.9 MB)\n",
      "   ---------------------------------------- 0.0/51.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 5.0/51.9 MB 23.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 9.7/51.9 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 16.3/51.9 MB 25.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 20.7/51.9 MB 24.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 26.7/51.9 MB 24.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 28.0/51.9 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 33.0/51.9 MB 22.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 39.3/51.9 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 44.3/51.9 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 49.0/51.9 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  51.9/51.9 MB 22.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.9/51.9 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: wrapt, tensorflow-estimator, pyasn1, protobuf, oauthlib, keras, gast, cachetools, rsa, requests-oauthlib, pyasn1-modules, jaxlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.6.0\n",
      "    Uninstalling keras-3.6.0:\n",
      "      Successfully uninstalled keras-3.6.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow_intel 2.18.0\n",
      "    Uninstalling tensorflow_intel-2.18.0:\n",
      "      Successfully uninstalled tensorflow_intel-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed cachetools-5.5.2 gast-0.4.0 google-auth-2.40.3 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 oauthlib-3.3.1 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 wrapt-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~ensorflow'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pykospacing 0.5 requires tensorflow>=2.16.2, but you have tensorflow 2.12.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Numpy 버전 맞춘 후 TensorFlow 재설치 -> 왜이렇게 오래 걸리지\n",
    "!pip install --upgrade --force-reinstall numpy==1.23.5\n",
    "!pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d743124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to c:\\users\\public\\documents\\estsoft\\creatortemp\\pip-req-build-9mj77z7a\n",
      "  Resolved https://github.com/haven-jeon/PyKoSpacing.git to commit b32a889cbd10b006d2f4aba118f0cd5b677e2979\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow>=2.16.2 (from pykospacing==0.5)\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from pykospacing==0.5) (3.12.1)\n",
      "Collecting argparse>=1.1.0 (from pykospacing==0.5)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from h5py>=3.10.0->pykospacing==0.5) (1.23.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (1.67.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow>=2.16.2->pykospacing==0.5)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow>=2.16.2->pykospacing==0.5)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy>=1.19.3 (from h5py>=3.10.0->pykospacing==0.5)\n",
      "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.16.2->pykospacing==0.5)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorflow>=2.16.2->pykospacing==0.5) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.16.2->pykospacing==0.5) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow>=2.16.2->pykospacing==0.5) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.16.2->pykospacing==0.5) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.16.2->pykospacing==0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.16.2->pykospacing==0.5) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.16.2->pykospacing==0.5) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\wooll\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\wooll\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.16.2->pykospacing==0.5) (0.1.0)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-win_amd64.whl (375.9 MB)\n",
      "   ---------------------------------------- 0.0/375.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.0/375.9 MB 21.5 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 10.0/375.9 MB 23.0 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 13.6/375.9 MB 19.5 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 17.8/375.9 MB 19.7 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 23.1/375.9 MB 20.6 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 27.5/375.9 MB 20.5 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 31.2/375.9 MB 20.2 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 35.7/375.9 MB 20.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 39.3/375.9 MB 20.3 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 40.6/375.9 MB 18.9 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 45.4/375.9 MB 19.0 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 51.1/375.9 MB 19.6 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 56.9/375.9 MB 20.4 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 61.6/375.9 MB 20.6 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 66.1/375.9 MB 20.6 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 72.6/375.9 MB 21.1 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 79.4/375.9 MB 21.7 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 85.7/375.9 MB 22.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 90.2/375.9 MB 22.1 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 95.9/375.9 MB 22.4 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 101.7/375.9 MB 22.5 MB/s eta 0:00:13\n",
      "   ----------- --------------------------- 107.7/375.9 MB 22.8 MB/s eta 0:00:12\n",
      "   ----------- --------------------------- 112.7/375.9 MB 22.8 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 116.4/375.9 MB 23.0 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 119.0/375.9 MB 22.2 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 124.5/375.9 MB 22.3 MB/s eta 0:00:12\n",
      "   ------------- ------------------------- 130.0/375.9 MB 22.4 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 135.8/375.9 MB 22.6 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 140.8/375.9 MB 22.7 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 145.8/375.9 MB 22.7 MB/s eta 0:00:11\n",
      "   --------------- ----------------------- 151.8/375.9 MB 22.9 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 157.0/375.9 MB 22.9 MB/s eta 0:00:10\n",
      "   ---------------- ---------------------- 162.8/375.9 MB 23.0 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 167.2/375.9 MB 23.0 MB/s eta 0:00:10\n",
      "   ----------------- --------------------- 171.2/375.9 MB 22.9 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 175.6/375.9 MB 22.9 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 180.4/375.9 MB 22.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 185.1/375.9 MB 22.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 189.8/375.9 MB 22.8 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 193.5/375.9 MB 22.7 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 198.4/375.9 MB 22.7 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 203.4/375.9 MB 22.7 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 207.6/375.9 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 212.1/375.9 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 216.0/375.9 MB 22.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 219.4/375.9 MB 22.4 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 222.8/375.9 MB 22.3 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 226.2/375.9 MB 22.1 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 230.4/375.9 MB 22.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 234.1/375.9 MB 22.0 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 239.1/375.9 MB 22.0 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 244.1/375.9 MB 21.9 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 248.3/375.9 MB 21.7 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 254.0/375.9 MB 21.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 256.6/375.9 MB 21.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 262.1/375.9 MB 21.5 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 267.6/375.9 MB 21.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 271.6/375.9 MB 21.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 277.3/375.9 MB 21.3 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 283.4/375.9 MB 21.2 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 289.7/375.9 MB 21.2 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 295.4/375.9 MB 21.3 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 300.2/375.9 MB 21.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 304.3/375.9 MB 21.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 308.5/375.9 MB 21.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 314.6/375.9 MB 21.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 320.9/375.9 MB 21.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 326.9/375.9 MB 21.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 330.3/375.9 MB 21.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 333.2/375.9 MB 21.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 338.4/375.9 MB 21.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 341.6/375.9 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 348.1/375.9 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 354.4/375.9 MB 21.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 361.2/375.9 MB 21.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  367.0/375.9 MB 21.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  373.6/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.9 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 375.9/375.9 MB 17.8 MB/s eta 0:00:00\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl (209 kB)\n",
      "Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -------------------------------------- - 5.2/5.5 MB 29.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 18.7 MB/s eta 0:00:00\n",
      "Installing collected packages: argparse, numpy, tensorboard, ml-dtypes, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "Successfully installed argparse-1.4.0 keras-3.10.0 ml-dtypes-0.5.1 numpy-2.1.3 tensorboard-2.19.0 tensorflow-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/haven-jeon/PyKoSpacing.git 'C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\pip-req-build-9mj77z7a'\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~=mpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wooll\\anaconda3\\Lib\\site-packages\\~l_dtypes'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 3.10.0 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.19.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 띄어쓰기 패키지 설치\n",
    "# pykospacing 재설치\n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b3d51cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting keras==2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorboard==2.12.0\n",
      "  Downloading tensorboard-2.12.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow==2.12.0\n",
      "  Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard==2.12.0)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard==2.12.0)\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard==2.12.0)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.12.0)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard==2.12.0)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorboard==2.12.0)\n",
      "  Downloading protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard==2.12.0)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard==2.12.0)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.12.0)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.12.0)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard==2.12.0)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel>=0.26 (from tensorboard==2.12.0)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow==2.12.0)\n",
      "  Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading h5py-3.14.0-cp311-cp311-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting packaging (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf>=3.19.6 (from tensorboard==2.12.0)\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting six>=1.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.12.0)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.12.0)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard==2.12.0)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.12.0)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard==2.12.0)\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard==2.12.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard==2.12.0)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard==2.12.0)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard==2.12.0)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.6.2-cp311-cp311-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.6.1-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.6.0-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.5.3-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.5.1-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.5.0-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.38-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.36-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "  Using cached jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.35-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.34-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.33-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.31-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Using cached jaxlib-0.4.30-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting scipy>=1.9 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.12.0)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=1.9 (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow==2.12.0)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 11.9 MB/s eta 0:00:00\n",
      "Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 4.2/4.3 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 14.9 MB/s eta 0:00:00\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "   ---------------------------------------- 0.0/781.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 781.3/781.3 kB 34.6 MB/s eta 0:00:00\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp311-cp311-win_amd64.whl (105 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 16.7 MB/s eta 0:00:00\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached jaxlib-0.4.30-cp311-cp311-win_amd64.whl (51.9 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp311-cp311-win_amd64.whl (209 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading scipy-1.15.3-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "   ---------------------------------------- 0.0/41.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.7/41.2 MB 23.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.5/41.2 MB 26.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 18.4/41.2 MB 28.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 24.9/41.2 MB 29.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 30.4/41.2 MB 28.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.2/41.2 MB 28.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.2 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.2/41.2 MB 24.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, opt-einsum, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset_normalizer, certifi, cachetools, absl-py, werkzeug, scipy, rsa, requests, pyasn1-modules, ml_dtypes, h5py, google-pasta, astunparse, requests-oauthlib, jaxlib, google-auth, jax, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 18.1.1\n",
      "    Uninstalling libclang-18.1.1:\n",
      "      Successfully uninstalled libclang-18.1.1\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 24.3.25\n",
      "    Uninstalling flatbuffers-24.3.25:\n",
      "      Successfully uninstalled flatbuffers-24.3.25\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\wooll\\anaconda3\\Lib\\site-packages)\n",
      "error: uninstall-no-record-file\n",
      "\n",
      "× Cannot uninstall wheel None\n",
      "╰─> The package's contents are unknown: no RECORD file was found for wheel.\n",
      "\n",
      "hint: You might be able to recover from this via: pip install --force-reinstall --no-deps wheel==0.44.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall \\\n",
    "  numpy==1.23.5 \\\n",
    "  keras==2.12.0 \\\n",
    "  tensorboard==2.12.0 \\\n",
    "  tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치 후 콘솔 재시작\n",
    "# 버전 제대로 맞춰졌는지 확인\n",
    "import numpy as np, tensorflow as tf, keras, tensorboard\n",
    "print(np.__version__, tf.__version__, keras.__version__, tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "008a8db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Couldn't build proto file into descriptor pool: duplicate symbol 'tensorflow.CoordinatedJob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykospacing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Spacing\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhanspell\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spell_checker\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pykospacing\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykospacing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkospacing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pykospacing\\kospacing.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFSMLayer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykospacing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encoding_and_padding, load_vocab\n\u001b[0;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpacing\u001b[39m\u001b[38;5;124m'\u001b[39m, ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_debug_info_pb2\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\compiler\\xla\\tsl\\protobuf\\coordination_config_pb2.py:16\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m \u001b[43m_descriptor_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAddSerializedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m*xla/tsl/protobuf/coordination_config.proto\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtensorflow\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0e\u001b[39;49;00m\u001b[38;5;130;43;01m\\x43\u001b[39;49;00m\u001b[38;5;124;43moordinatedJob\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x04\u001b[39;49;00m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x11\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mnum_tasks\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x02\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x05\u001b[39;49;00m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;130;43;01m\\xf7\u001b[39;49;00m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x19\u001b[39;49;00m\u001b[38;5;130;43;01m\\x43\u001b[39;49;00m\u001b[38;5;124;43moordinationServiceConfig\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x14\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;124;43mservice_type\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x16\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0e\u001b[39;49;00m\u001b[38;5;124;43mservice_leader\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x02\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1b\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x13\u001b[39;49;00m\u001b[38;5;130;43;01m\\x65\u001b[39;49;00m\u001b[38;5;124;43mnable_health_check\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m&\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1e\u001b[39;49;00m\u001b[38;5;130;43;01m\\x63\u001b[39;49;00m\u001b[38;5;124;43mluster_register_timeout_in_ms\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x04\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1d\u001b[39;49;00m\u001b[38;5;130;43;01m\\x63\u001b[39;49;00m\u001b[38;5;124;43mluster_register_with_barrier\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0e\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1f\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x17\u001b[39;49;00m\u001b[38;5;124;43mheartbeat_timeout_in_ms\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x05\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x38\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x14\u001b[39;49;00m\u001b[38;5;130;43;01m\\x63\u001b[39;49;00m\u001b[38;5;124;43moordinated_job_list\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x0b\u001b[39;49;00m\u001b[38;5;130;43;01m\\x32\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1a\u001b[39;49;00m\u001b[38;5;124;43m.tensorflow.CoordinatedJob\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m&\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1e\u001b[39;49;00m\u001b[38;5;124;43mshutdown_barrier_timeout_in_ms\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x07\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43magent_destruction_without_shutdown\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x10\u001b[39;49;00m\u001b[38;5;124;43mrecoverable_jobs\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mallow_new_incarnation_to_reconnect\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0b\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x15\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43mforce_disable\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m&poll_for_error_from_service_at_startup\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;124;43mJ\u001b[39;49m\u001b[38;5;130;43;01m\\x04\u001b[39;49;00m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x06\u001b[39;49;00m\u001b[38;5;130;43;01m\\x10\u001b[39;49;00m\u001b[38;5;130;43;01m\\x07\u001b[39;49;00m\u001b[38;5;130;43;01m\\x42\u001b[39;49;00m\u001b[38;5;124;43mWZUgithub.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_protob\u001b[39;49m\u001b[38;5;130;43;01m\\x06\u001b[39;49;00m\u001b[38;5;124;43mproto3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m _builder\u001b[38;5;241m.\u001b[39mBuildMessageAndEnumDescriptors(DESCRIPTOR, \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m     19\u001b[0m _builder\u001b[38;5;241m.\u001b[39mBuildTopDescriptorsAndMessages(DESCRIPTOR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxla.tsl.protobuf.coordination_config_pb2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mglobals\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: Couldn't build proto file into descriptor pool: duplicate symbol 'tensorflow.CoordinatedJob'"
     ]
    }
   ],
   "source": [
    "from pykospacing import Spacing\n",
    "from hanspell import spell_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(r'C:\\Users\\wooll\\OneDrive\\문서\\GitHub\\-\\dataset\\NIKL_NP.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eafafa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warning] hanspell KeyError: 'result'. 원문을 반환합니다.\n",
      "오늘 날씨가 넘 좋아요~\n"
     ]
    }
   ],
   "source": [
    "def correct_spelling(text):\n",
    "    \"\"\"\n",
    "    네이버 맞춤법 검사기(hanspell)를 사용해 맞춤법을 교정합니다.\n",
    "    API 오류나 예상치 못한 응답 구조 변경 시 예외를 잡아\n",
    "    원문(text)을 그대로 반환하도록 합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 맞춤법 검사 실행\n",
    "        checked = spell_checker.check(text)\n",
    "        # checked.checked 에 교정된 문자열이 들어 있습니다.\n",
    "        return checked.checked\n",
    "    except KeyError as e:\n",
    "        # 응답에 'result' 키가 없어서 KeyError 발생 시\n",
    "        print(f\"[warning] hanspell KeyError: {e}. 원문을 반환합니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        # 그 외 네트워크 오류, JSON 파싱 오류 등 모든 예외를 포착\n",
    "        print(f\"[warning] hanspell failed ({type(e).__name__}): {e}. 원문을 반환합니다.\")\n",
    "        return text\n",
    "\n",
    "# 적용 예\n",
    "raw = \"오늘 날씨가 넘 좋아요~\"\n",
    "print(correct_spelling(raw))  # → \"오늘 날씨가 너무 좋아요~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6a291f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Spacing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 띄어쓰기 교정기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spacing \u001b[38;5;241m=\u001b[39m \u001b[43mSpacing\u001b[49m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_spacing\u001b[39m(text):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Spacing' is not defined"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 교정기\n",
    "spacing = Spacing()\n",
    "def correct_spacing(text):\n",
    "    try:\n",
    "        return spacing(text)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "# 맞춤법 교정기 with 예외 처리\n",
    "def correct_spelling(text):\n",
    "    try:\n",
    "        checked = spell_checker.check(text)\n",
    "        return checked.checked\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "# 특수문자·반복문자 정제\n",
    "def clean_punctuation(text):\n",
    "    # 연속된 .,!? 제거\n",
    "    text = re.sub(r'([!?\\.])\\1+', r'\\1', text)\n",
    "    # 불필요 기호 제거\n",
    "    text = re.sub(r'[※☆★♡♪]', ' ', text)\n",
    "    # 다중 공백 정리\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 통합 정규화 함수\n",
    "def normalize_text(text):\n",
    "    text = str(text)\n",
    "    text = correct_spacing(text)         # 1) 띄어쓰기\n",
    "    text = correct_spelling(text)        # 2) 맞춤법\n",
    "    text = clean_punctuation(text)       # 3) 특수문자\n",
    "    return text\n",
    "\n",
    "# 기존 한글 토크나이저\n",
    "def tokenize_kr(text):\n",
    "    text = re.sub(r'[^가-힣\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = re.findall(r'[가-힣]{2,}', text)\n",
    "    return [t for t in tokens if t not in stopwords_kr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(r'C:\\Users\\wooll\\OneDrive\\문서\\GitHub\\-\\dataset\\NIKL_NP.csv')\n",
    "df['sentence'] = df['sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9af126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 문장에 정규화 적용\n",
    "df['normalized'] = df['sentence'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화된 문장에 토큰화 적용\n",
    "df['tokens'] = df['normalized'].apply(tokenize_kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "print(df[['sentence','normalized','tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0fd70b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokens'] -> 한글 토큰 리스트 준비되어있는 상태\n",
    "\n",
    "# 빈도 기반 감정 어근 후보 뽑기\n",
    "unlab_tokens = df.loc[~df['has_emotion'], 'tokens'].tolist() # 라벨 없는 문장들의 토크\n",
    "all_unlab = list(chain.from_iterable(unlab_tokens))\n",
    "freq = Counter(all_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2650ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 1000개 토큰 후보로 진행 -> 살펴 본 결과 감정을 나타내는 토큰이 많이 없음\n",
    "# cand_freq = pd.DataFrame(freq.most_common(1000), columns=['token','count'])\n",
    "# cand_freq.to_csv('candidates_freq2.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 하위 1000개 토큰 후보로 진행 -> most_common() 뒤집기\n",
    "bottom500 = freq.most_common()[:-1001:-1]\n",
    "cand_least = pd.DataFrame(bottom500, columns=['token','count'])\n",
    "cand_least.to_csv('candidates_least_freq.csv', index=False)\n",
    "\n",
    "# 엑셀로 열어서 직접 감정 달아줌 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 Label (인사이드 아웃 2 기반)\n",
    "# 기본 감정 감정 사전으로 했을 때 보다 라벨링 된 문장 비율 올라감. \n",
    "lexicon={\n",
    "  '기쁨': ['행복', '기쁘', '즐거', '환희', '기쁨'], # 기쁨이 (Joy)\n",
    "  '슬픔': ['슬픔', '슬프', '우울', '비통', '눈물', '상심'], # 슬픔이 (Sadness)\n",
    "  '버럭': ['분노', '화나', '열받', '격분', '버럭', '분개', '화가'], # 버럭이 (Anger)\n",
    "  '까칠': ['혐오', '싫', '역겹', '불쾌', '까칠', '거북', '싫증'], # 까칠이 (Disgust)\n",
    "  '소심': ['무섭', '두렵', '겁', '소심', '겁나다', '겁먹'], # 소심이 (Fear)\n",
    "  '불안': ['불안', '초조', '긴장', '떨리', '걱정', '안절부절'], # 불안이 (Anxiety)\n",
    "  '부러움': ['부럽', '부러움', '질투', '질투심', '시기'], # 부럽이 (Envy)\n",
    "  '당황': ['당황', '부끄러움', '민망', '어색', '당황스러움'], # 당황이 (Embarrassment)\n",
    "  '따분': ['따분', '지루', '귀찮', '권태', '심심', '지루함'] # 따분이 (Ennui)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce96f4",
   "metadata": {},
   "source": [
    "## 2. 문맥 반영 모델 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0d8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
